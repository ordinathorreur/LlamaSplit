# Sample configuration file for Llama servers

# Flags to enable or disable Vulkan and CUDA
ENABLE_VULKAN=true
ENABLE_CUDA=true

# Vulkan configuration
VULKAN_DEVICE=0
VULKAN_MODEL_PATH=/path/to/vulkan/model/file.gguf
VULKAN_GRAMMAR_FILE=/path/to/vulkan/grammar/file.gbnf
VULKAN_HOST=127.0.0.1
VULKAN_PORT=8081
VULKAN_CONTEXT=128000

# CUDA configuration
CUDA_VISIBLE_DEVICES=0
CUDA_MODEL_PATH=/path/to/cuda/model/file.gguf
CUDA_GRAMMAR_FILE=/path/to/cuda/grammar/file.gbnf
CUDA_HOST=127.0.0.1
CUDA_PORT=8082
CUDA_CONTEXT=32000